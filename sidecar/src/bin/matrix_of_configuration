4 things:
- 1. sidebar chat (70B)
- 2. inline chat (7B)
- 3. tab completion (7B)
- 4. export to codebase (GPT3.5)
- 4. next edit prediction (not doing right now)

# The providers
- ollama
- together.ai
- replicate?
- perplexity?
- OpenAI
- Azure OpenAI

# What features work with which LLM?
[llm, provider, features, does_it_work?]
[llama70B, ollama, sidebar, works]
[llama7B, ollama, export_to_codebase, does_not_work]


# Finetuned models and what do we do about them?
If this is lora finetuned for a usecase, we can load and unload the lora config and have the same mode
do different things.

# Any model out there or finetuned model?
- (skcd) Any model out there
- (ghost) only finetuned model and go from there

# Benchmarks (interesting blogs)
- Different features and how they work with models: sidebar chat (noo.....)
- inline chat: benchmarks on different models

# UX work
- sensible defaults (what can we do?) + UX to make this good.0
- sensible defaults: OpenAI, or finetuned models
- advanced settings: BYOM


# What we want to do today?
- UX support for configuring different parts of the editor (simple one)
- get our finetuned model to run with ollama so we can start testing it
- no changes to prompt yet (we will get to it later)