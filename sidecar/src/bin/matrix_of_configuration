4 things:
- 1. sidebar chat (70B)
- 2. inline chat (7B)
- 3. tab completion (7B)
- 4. export to codebase (GPT3.5)
- 4. next edit prediction (not doing right now)

# The providers
- ollama
- together.ai
- replicate?
- perplexity?
- OpenAI
- Azure OpenAI

# What features work with which LLM?
[llm, provider, features, does_it_work?]
[llama70B, ollama, sidebar, works]
[llama7B, ollama, export_to_codebase, does_not_work]


# Finetuned models and what do we do about them?
If this is lora finetuned for a usecase, we can load and unload the lora config and have the same mode
do different things.

# Any model out there or finetuned model?
- (skcd) Any model out there
- (ghost) only finetuned model and go from there

# Benchmarks (interesting blogs)
- Different features and how they work with models: sidebar chat (noo.....)
- inline chat: benchmarks on different models

# UX work
- sensible defaults (what can we do?) + UX to make this good.0
- sensible defaults: OpenAI, or finetuned models
- advanced settings: BYOM


# What we want to do today?
- UX support for configuring different parts of the editor (simple one)
- get our finetuned model to run with ollama so we can start testing it [done]
- no changes to prompt yet (we will get to it later)



Either we set a single model for everything, or we can set different models for different parts.
- sidebar chat (add model name to the config)
- inline chat (add model name to the config for this)

Need to think
- tab completion


- sidebar chat (GPT4)
- reranking of code snippets (uses a mix of GPT4 and GPT4_Turbo, GPT3.5)
- inline chat (uses GPT 3.5)
- export to codebase (uses GPT 3.5)


- If we are using the inline chat, what if we exceed the context length? we still need a reranker of sorts

<start>
- Change the inline chat interface
</start>
